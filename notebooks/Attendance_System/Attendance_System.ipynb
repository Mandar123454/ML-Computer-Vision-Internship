{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0IbJfzmtixo8"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3ca8ebfa"
      },
      "source": [
        "# Task\n",
        "Create a machine learning model for an attendance system that detects and identifies students, marks their attendance (present/absent), detects their emotions, and stores the data (student ID, attendance status, emotion, time of identification) in an Excel or CSV file. The system should only be active between 9:30 AM and 10:00 AM. Train the model using a custom dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "258785da"
      },
      "source": [
        "## Data collection and preparation\n",
        "\n",
        "### Subtask:\n",
        "Gather a dataset of student images, including variations in lighting, poses, and expressions. This dataset should also include labels for student identity and emotions.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "42275c04"
      },
      "source": [
        "**Reasoning**:\n",
        "Define the dataset requirements, outline a data collection strategy, describe the labeling process, and detail data organization and storage.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "64c387fd",
        "outputId": "41be947b-7c4a-4254-9473-72f43bdd546f"
      },
      "source": [
        "# 1. Dataset Requirements:\n",
        "#    - Number of images per student: Aim for at least 50-100 images per student to ensure robustness.\n",
        "#    - Variations:\n",
        "#        - Lighting: Collect images in different lighting conditions (bright, dim, natural light, artificial light).\n",
        "#        - Pose: Capture images from various angles (front, side, slightly tilted).\n",
        "#        - Expression: Include a range of emotions (neutral, happy, sad, angry, surprised, etc.).\n",
        "#    - Labeling format: Each image filename or associated metadata should include the student ID and the emotion displayed.\n",
        "\n",
        "# 2. Data Collection Strategy:\n",
        "#    - Method: The primary method will be capturing images using a camera in a controlled environment simulating the classroom setting.\n",
        "#    - Process:\n",
        "#        - Schedule sessions with students to capture images.\n",
        "#        - Ensure ethical considerations and obtain necessary consent.\n",
        "#        - Vary the lighting, pose, and ask students to display different emotions during the session.\n",
        "#    - Alternative (if applicable and permissible): Explore using existing, publicly available datasets for emotion recognition if they align with the required emotions and ethical guidelines. However, student identification will require custom data.\n",
        "\n",
        "# 3. Labeling Process:\n",
        "#    - Tools: Utilize image labeling tools such as Labelbox, VGG Image Annotator (VIA), or even custom scripts for efficient labeling.\n",
        "#    - Process:\n",
        "#        - For each image, assign the correct student ID.\n",
        "#        - For each image, assign the dominant emotion displayed from a predefined set of emotions.\n",
        "#        - Implement quality control to ensure labeling accuracy, potentially involving multiple annotators for verification.\n",
        "\n",
        "# 4. Data Organization and Storage:\n",
        "#    - Directory Structure:\n",
        "#        - A main directory for the dataset (e.g., `student_attendance_dataset`).\n",
        "#        - Subdirectories within the main directory for each student, named by their student ID (e.g., `student_attendance_dataset/student_001`).\n",
        "#        - Within each student's directory, further subdirectories for different emotions (e.g., `student_attendance_dataset/student_001/happy`).\n",
        "#    - File Naming Convention:\n",
        "#        - Image files should be named to include information about the pose and lighting if possible, in addition to a unique identifier (e.g., `student_001_happy_frontal_bright_001.jpg`).\n",
        "#    - Storage: Store the dataset on a reliable storage medium, such as cloud storage or a local server with backups.\n",
        "\n",
        "print(\"Dataset requirements, collection strategy, labeling process, and data organization detailed.\")"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset requirements, collection strategy, labeling process, and data organization detailed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "deb11250"
      },
      "source": [
        "## Face detection and recognition model training\n",
        "\n",
        "### Subtask:\n",
        "Train a model to detect faces in images and recognize individual students. This could involve using pre-trained models or training a custom model.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e9bb9689"
      },
      "source": [
        "**Reasoning**:\n",
        "The subtask is to train a model for face detection and recognition. I will use the `mtcnn` library for face detection as it is a robust and efficient choice, and then use `FaceNet` which is a pre-trained model for face recognition. I will then extract face embeddings using `FaceNet` and train a KNN classifier on these embeddings to recognize students. Finally, I will save the trained classifier. This covers steps 1, 2, 3, 6, 7, and 8 of the instructions. Since we don't have the actual dataset, I will use dummy data to demonstrate the process.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "386e22d4",
        "outputId": "26fd702a-f48d-4a12-a4d9-a489ea0ec345"
      },
      "source": [
        "# Install necessary libraries\n",
        "!pip install mtcnn tensorflow numpy scikit-learn\n",
        "\n",
        "import os\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "import cv2\n",
        "\n",
        "# Dummy data creation (replace with your actual dataset loading)\n",
        "# In a real scenario, you would load images from your dataset directory\n",
        "# and extract faces and labels.\n",
        "num_students = 5\n",
        "images_per_student = 10\n",
        "img_height, img_width = 160, 160\n",
        "dummy_images = np.random.rand(num_students * images_per_student, img_height, img_width, 3) * 255\n",
        "dummy_labels = np.repeat(np.arange(num_students), images_per_student)\n",
        "\n",
        "# 1. Choose and load a suitable face detection model (MTCNN)\n",
        "detector = MTCNN()\n",
        "\n",
        "# 2. Load a suitable face recognition model (FaceNet)\n",
        "# Load pre-trained FaceNet model from TensorFlow Hub (or other source)\n",
        "# For demonstration, we'll simulate loading a model.\n",
        "# In a real scenario, you would load the actual FaceNet model.\n",
        "# Example using a simplified model for demonstration:\n",
        "from tensorflow.keras.applications import InceptionResNetV2\n",
        "from tensorflow.keras.layers import Input, Dense, Flatten, Lambda\n",
        "from tensorflow.keras.models import Model\n",
        "import tensorflow.keras.backend as K\n",
        "\n",
        "def preprocess(x):\n",
        "    return tf.image.resize(x, (160, 160))\n",
        "\n",
        "def create_facenet_model():\n",
        "    base_model = InceptionResNetV2(weights='imagenet', include_top=False, input_shape=(160, 160, 3))\n",
        "    x = Flatten()(base_model.output)\n",
        "    x = Dense(128, activation=None)(x) # FaceNet uses a Dense layer for embeddings\n",
        "    # Add L2 normalization layer\n",
        "    x = Lambda(lambda embeddings: K.l2_normalize(embeddings, axis=-1))(x)\n",
        "    model = Model(inputs=base_model.input, outputs=x)\n",
        "    return model\n",
        "\n",
        "# This will download weights the first time\n",
        "try:\n",
        "    facenet_model = create_facenet_model()\n",
        "except:\n",
        "    print(\"Could not load InceptionResNetV2 weights. This is expected in some environments without internet access.\")\n",
        "    # Create a dummy model if weights cannot be loaded\n",
        "    input_tensor = Input(shape=(160, 160, 3))\n",
        "    x = Flatten()(input_tensor)\n",
        "    output_tensor = Dense(128)(x)\n",
        "    facenet_model = Model(inputs=input_tensor, outputs=output_tensor)\n",
        "\n",
        "\n",
        "# 6. Extract face embeddings from the detected faces\n",
        "face_embeddings = []\n",
        "student_ids = []\n",
        "\n",
        "for i in range(dummy_images.shape[0]):\n",
        "    img = dummy_images[i]\n",
        "    # Convert to uint8 for MTCNN\n",
        "    img_uint8 = img.astype('uint8')\n",
        "    results = detector.detect_faces(img_uint8)\n",
        "\n",
        "    if results:\n",
        "        # Assume the first detected face is the student for simplicity\n",
        "        x, y, width, height = results[0]['box']\n",
        "        face = img_uint8[y:y+height, x:x+width]\n",
        "\n",
        "        # Preprocess face for FaceNet\n",
        "        face = cv2.resize(face, (160, 160))\n",
        "        face = np.expand_dims(face, axis=0) # Add batch dimension\n",
        "        face = preprocess(face) # Apply FaceNet specific preprocessing if any\n",
        "\n",
        "        # Get embedding\n",
        "        embedding = facenet_model.predict(face)\n",
        "        face_embeddings.append(embedding[0])\n",
        "        student_ids.append(dummy_labels[i])\n",
        "\n",
        "face_embeddings = np.array(face_embeddings)\n",
        "student_ids = np.array(student_ids)\n",
        "\n",
        "# 7. Train a classifier (KNN) on the face embeddings\n",
        "if len(face_embeddings) > 0:\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=min(len(np.unique(student_ids)), 5)) # Use min of number of classes or 5\n",
        "    knn_classifier.fit(face_embeddings, student_ids)\n",
        "    print(\"KNN classifier trained successfully.\")\n",
        "else:\n",
        "    print(\"No faces detected in dummy data. Cannot train classifier.\")\n",
        "    knn_classifier = None\n",
        "\n",
        "\n",
        "# 8. Save the trained face detection model and the trained recognition model/classifier\n",
        "# MTCNN does not have a save method in this implementation, it's class based.\n",
        "# FaceNet model can be saved if it was trained or modified. Here we use a pre-trained idea.\n",
        "# Save the trained KNN classifier\n",
        "import joblib\n",
        "\n",
        "if knn_classifier:\n",
        "    joblib.dump(knn_classifier, 'knn_classifier.pkl')\n",
        "    print(\"KNN classifier saved as knn_classifier.pkl\")\n",
        "\n",
        "# In a real application, you would also consider saving the FaceNet model if fine-tuned\n",
        "# facenet_model.save('facenet_model.h5') # Example save\n",
        "\n",
        "print(\"Face detection and recognition models processed and classifier trained and saved.\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting mtcnn\n",
            "  Downloading mtcnn-1.0.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.4.2 in /usr/local/lib/python3.12/dist-packages (from mtcnn) (1.5.2)\n",
            "Collecting lz4>=4.3.3 (from mtcnn)\n",
            "  Downloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.0)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.16.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Downloading mtcnn-1.0.0-py3-none-any.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m36.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading lz4-4.4.4-cp312-cp312-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m38.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: lz4, mtcnn\n",
            "Successfully installed lz4-4.4.4 mtcnn-1.0.0\n",
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/inception_resnet_v2/inception_resnet_v2_weights_tf_dim_ordering_tf_kernels_notop.h5\n",
            "\u001b[1m219055592/219055592\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 0us/step\n",
            "No faces detected in dummy data. Cannot train classifier.\n",
            "Face detection and recognition models processed and classifier trained and saved.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b964cd22"
      },
      "source": [
        "**Reasoning**:\n",
        "The previous attempt to train the classifier failed because no faces were detected in the dummy data. This is expected with random noise as input to the face detector. To successfully demonstrate the training of the KNN classifier, I will modify the dummy data to contain simple \"face-like\" patterns that MTCNN is likely to detect. This will allow the subsequent steps of extracting embeddings and training the classifier to proceed as intended.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "063067cd",
        "outputId": "394cb40d-99b8-47c1-d16b-c7654617621d"
      },
      "source": [
        "# Modify dummy data to include simple patterns that MTCNN might detect\n",
        "# This is a simplification for demonstration purposes. Real faces are complex.\n",
        "def create_dummy_face(img_height, img_width):\n",
        "    img = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
        "    center_x, center_y = img_width // 2, img_height // 2\n",
        "    # Draw a circle for the head\n",
        "    cv2.circle(img, (center_x, center_y), min(img_height, img_width) // 3, (255, 255, 255), -1)\n",
        "    # Draw eyes\n",
        "    eye_radius = max(1, min(img_height, img_width) // 20)\n",
        "    eye_offset_x = min(img_width // 6, center_x - eye_radius - 1)\n",
        "    eye_offset_y = min(img_height // 6, center_y - eye_radius - 1)\n",
        "    cv2.circle(img, (center_x - eye_offset_x, center_y - eye_offset_y), eye_radius, (0, 0, 0), -1)\n",
        "    cv2.circle(img, (center_x + eye_offset_x, center_y - eye_offset_y), eye_radius, (0, 0, 0), -1)\n",
        "    # Draw mouth (simple line)\n",
        "    mouth_width = min(img_width // 4, center_x - 10)\n",
        "    mouth_height_offset = min(img_height // 8, center_y - 10)\n",
        "    cv2.line(img, (center_x - mouth_width // 2, center_y + mouth_height_offset),\n",
        "             (center_x + mouth_width // 2, center_y + mouth_height_offset), (0, 0, 0), max(1, min(img_height, img_width) // 30))\n",
        "    return img\n",
        "\n",
        "# Create dummy images with simple face patterns\n",
        "dummy_images_with_faces = np.array([create_dummy_face(img_height, img_width)\n",
        "                                    for _ in range(num_students * images_per_student)])\n",
        "dummy_labels = np.repeat(np.arange(num_students), images_per_student)\n",
        "\n",
        "\n",
        "# Use the existing detector and facenet_model from the previous cell\n",
        "\n",
        "# Extract face embeddings from the detected faces using the new dummy data\n",
        "face_embeddings = []\n",
        "student_ids = []\n",
        "\n",
        "for i in range(dummy_images_with_faces.shape[0]):\n",
        "    img = dummy_images_with_faces[i]\n",
        "    results = detector.detect_faces(img)\n",
        "\n",
        "    if results:\n",
        "        # Assume the first detected face is the student for simplicity\n",
        "        x, y, width, height = results[0]['box']\n",
        "        face = img[y:y+height, x:x+width]\n",
        "\n",
        "        # Preprocess face for FaceNet\n",
        "        face = cv2.resize(face, (160, 160))\n",
        "        face = np.expand_dims(face, axis=0) # Add batch dimension\n",
        "        face = preprocess(face) # Apply FaceNet specific preprocessing if any\n",
        "\n",
        "        # Get embedding\n",
        "        embedding = facenet_model.predict(face)\n",
        "        face_embeddings.append(embedding[0])\n",
        "        student_ids.append(dummy_labels[i])\n",
        "\n",
        "face_embeddings = np.array(face_embeddings)\n",
        "student_ids = np.array(student_ids)\n",
        "\n",
        "# Train a classifier (KNN) on the face embeddings\n",
        "if len(face_embeddings) > 0 and len(np.unique(student_ids)) > 1:\n",
        "    knn_classifier = KNeighborsClassifier(n_neighbors=min(len(np.unique(student_ids)), 5))\n",
        "    knn_classifier.fit(face_embeddings, student_ids)\n",
        "    print(\"KNN classifier trained successfully.\")\n",
        "else:\n",
        "    print(\"Could not train classifier. Ensure faces were detected and there is more than one student ID.\")\n",
        "    knn_classifier = None\n",
        "\n",
        "# Save the trained KNN classifier\n",
        "import joblib\n",
        "\n",
        "if knn_classifier:\n",
        "    joblib.dump(knn_classifier, 'knn_classifier.pkl')\n",
        "    print(\"KNN classifier saved as knn_classifier.pkl\")\n",
        "\n",
        "print(\"Face detection and recognition models processed and classifier trained and saved using modified dummy data.\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Could not train classifier. Ensure faces were detected and there is more than one student ID.\n",
            "Face detection and recognition models processed and classifier trained and saved using modified dummy data.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8eb2b8a7"
      },
      "source": [
        "## Emotion detection model training\n",
        "\n",
        "### Subtask:\n",
        "Train a separate model to detect emotions from facial expressions. Similar to face recognition, pre-trained models can be used or a custom model can be trained.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bf78dd4a"
      },
      "source": [
        "**Reasoning**:\n",
        "Outline the steps for training an emotion detection model, including model choice, data preparation, training process, handling class imbalance, evaluation, and saving the model, as requested by the instructions. Since a real dataset is not available, this will be a conceptual outline.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a6098d03",
        "outputId": "febdb75e-6c0a-4776-f1fc-4f899d7a2b38"
      },
      "source": [
        "# 1. Choose a suitable model architecture and pre-trained model:\n",
        "#    - Architecture: Convolutional Neural Network (CNN) is well-suited for image-based tasks like emotion detection.\n",
        "#    - Pre-trained Model: Using a model pre-trained on a large facial expression dataset is recommended. Examples include:\n",
        "#        - VGG-16 or ResNet trained on AffectNet or FER2013 datasets.\n",
        "#        - Leveraging a library like Keras-Emotion (if available and suitable) that provides pre-trained models.\n",
        "#    - We will conceptually use a pre-trained CNN model, like a ResNet variant, with weights from a large emotion dataset.\n",
        "\n",
        "# 2. Data Preparation:\n",
        "#    - Assume the collected student dataset includes emotion labels for facial images.\n",
        "#    - Steps:\n",
        "#        - Load the images and corresponding emotion labels.\n",
        "#        - Resize images to a fixed size (e.g., 48x48 or 64x64) suitable for the chosen model.\n",
        "#        - Normalize pixel values (e.g., scale to [0, 1] or [-1, 1]).\n",
        "#        - Convert emotion labels to a numerical format (e.g., one-hot encoding).\n",
        "#        - Split the dataset into training and validation sets (e.g., 80% training, 20% validation).\n",
        "#        - Implement data augmentation (e.g., random rotations, flips, zooms) to increase the dataset size and improve model generalization.\n",
        "\n",
        "# 3. Training Process:\n",
        "#    - Model: Load the chosen pre-trained CNN model.\n",
        "#    - Fine-tuning: Replace the last few layers of the pre-trained model with new layers suitable for our number of emotion classes. Unfreeze some of the earlier layers for fine-tuning on the student data.\n",
        "#    - Loss Function: Categorical cross-entropy is suitable for multi-class classification problems like emotion detection.\n",
        "#    - Optimizer: Adam optimizer is a good choice for its adaptive learning rate properties.\n",
        "#    - Metrics: Monitor accuracy, precision, recall, and F1-score on the validation set to evaluate performance.\n",
        "#    - Training Loop: Iterate over epochs, feeding batches of training data to the model, calculating the loss, and updating the model weights using the optimizer. Evaluate on the validation set periodically.\n",
        "\n",
        "# 4. Handling Class Imbalance:\n",
        "#    - If some emotions are less frequent in the dataset, the model might be biased towards the majority classes.\n",
        "#    - Strategies:\n",
        "#        - Class weights: Assign higher weights to the loss for minority classes during training.\n",
        "#        - Data augmentation: Apply more aggressive augmentation to images of minority classes.\n",
        "#        - Resampling: Oversample minority classes or undersample majority classes (be cautious with undersampling as it can lead to loss of information).\n",
        "\n",
        "# 5. Evaluation:\n",
        "#    - Evaluate the trained model on the validation set using the chosen metrics (accuracy, precision, recall, F1-score).\n",
        "#    - Analyze the confusion matrix to understand which emotions are being misclassified.\n",
        "\n",
        "# 6. Saving the Model:\n",
        "#    - Save the trained model's architecture and weights in a standard format (e.g., Keras .h5 or TensorFlow SavedModel format).\n",
        "#    - This saved model can then be loaded later for making predictions in the attendance system.\n",
        "\n",
        "print(\"Conceptual outline for training an emotion detection model created.\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Conceptual outline for training an emotion detection model created.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5346774f"
      },
      "source": [
        "## Attendance tracking logic\n",
        "\n",
        "### Subtask:\n",
        "Develop the logic to mark attendance based on face recognition results. This will involve storing student information and their attendance status.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a0524a6"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a class to manage student attendance and initialize it with dummy student data, marking all students as absent.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 224
        },
        "id": "b853e117",
        "outputId": "daf4bdfc-66ab-433a-e33e-574decd4afb2"
      },
      "source": [
        "import pandas as pd\n",
        "from datetime import datetime\n",
        "\n",
        "class AttendanceManager:\n",
        "    def __init__(self, student_list):\n",
        "        \"\"\"\n",
        "        Initializes the AttendanceManager with a list of student IDs.\n",
        "        All students are initially marked as Absent.\n",
        "        \"\"\"\n",
        "        self.attendance_records = {\n",
        "            student_id: {'status': 'Absent', 'timestamp': None, 'emotion': None}\n",
        "            for student_id in student_list\n",
        "        }\n",
        "        self.present_students_today = set() # To track students already marked present\n",
        "\n",
        "    def mark_present(self, student_id, emotion):\n",
        "        \"\"\"\n",
        "        Marks a student as Present if they haven't been marked Present already.\n",
        "        Records the timestamp and detected emotion.\n",
        "        \"\"\"\n",
        "        if student_id in self.attendance_records and student_id not in self.present_students_today:\n",
        "            self.attendance_records[student_id]['status'] = 'Present'\n",
        "            self.attendance_records[student_id]['timestamp'] = datetime.now()\n",
        "            self.attendance_records[student_id]['emotion'] = emotion\n",
        "            self.present_students_today.add(student_id)\n",
        "            print(f\"Attendance marked for student {student_id} as Present.\")\n",
        "        elif student_id in self.present_students_today:\n",
        "            print(f\"Student {student_id} already marked as Present today.\")\n",
        "        else:\n",
        "            print(f\"Student {student_id} not found in the registered list.\")\n",
        "\n",
        "    def get_attendance_records(self):\n",
        "        \"\"\"\n",
        "        Returns the current attendance records.\n",
        "        \"\"\"\n",
        "        return self.attendance_records\n",
        "\n",
        "    def reset_attendance(self, student_list):\n",
        "        \"\"\"\n",
        "        Resets attendance for a new session, marking all students as Absent.\n",
        "        \"\"\"\n",
        "        print(\"Resetting attendance for a new session.\")\n",
        "        self.__init__(student_list)\n",
        "\n",
        "\n",
        "# Dummy list of student IDs for demonstration\n",
        "dummy_student_list = [f'student_{i:03d}' for i in range(num_students)] # Using num_students from previous cells\n",
        "\n",
        "# Initialize the AttendanceManager at the beginning of the attendance window\n",
        "attendance_system = AttendanceManager(dummy_student_list)\n",
        "\n",
        "# Display initial attendance records (all Absent)\n",
        "print(\"Initial Attendance Records:\")\n",
        "display(pd.DataFrame.from_dict(attendance_system.get_attendance_records(), orient='index'))"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Initial Attendance Records:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             status timestamp emotion\n",
              "student_000  Absent      None    None\n",
              "student_001  Absent      None    None\n",
              "student_002  Absent      None    None\n",
              "student_003  Absent      None    None\n",
              "student_004  Absent      None    None"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-73fafa4f-dada-40df-a7d3-a8b8c985f3ee\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>status</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>student_000</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_001</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_002</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_003</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_004</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-73fafa4f-dada-40df-a7d3-a8b8c985f3ee')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-73fafa4f-dada-40df-a7d3-a8b8c985f3ee button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-73fafa4f-dada-40df-a7d3-a8b8c985f3ee');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-c8b36ebd-7167-4f27-a04f-7daba1857648\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c8b36ebd-7167-4f27-a04f-7daba1857648')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-c8b36ebd-7167-4f27-a04f-7daba1857648 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "421b994c"
      },
      "source": [
        "**Reasoning**:\n",
        "Simulate the face recognition process and mark attendance for a few recognized students using the `AttendanceManager` class.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 345
        },
        "id": "e97d677a",
        "outputId": "17dff6a0-9696-4e61-ad56-99822ccac250"
      },
      "source": [
        "# Simulate recognizing a few students and their emotions\n",
        "recognized_students_data = [\n",
        "    {'student_id': 'student_001', 'emotion': 'Happy'},\n",
        "    {'student_id': 'student_003', 'emotion': 'Neutral'},\n",
        "    {'student_id': 'student_001', 'emotion': 'Surprised'}, # Recognizing the same student again\n",
        "    {'student_id': 'student_005', 'emotion': 'Happy'} # Recognizing an unrecognized student\n",
        "]\n",
        "\n",
        "print(\"\\nSimulating face recognition and marking attendance:\")\n",
        "for recognition_data in recognized_students_data:\n",
        "    student_id = recognition_data['student_id']\n",
        "    emotion = recognition_data['emotion']\n",
        "    attendance_system.mark_present(student_id, emotion)\n",
        "\n",
        "# Display attendance records after recognition\n",
        "print(\"\\nAttendance Records After Recognition:\")\n",
        "display(pd.DataFrame.from_dict(attendance_system.get_attendance_records(), orient='index'))\n",
        "\n",
        "# Simulate the end of the attendance window and resetting for a new session\n",
        "# In a real system, this would be triggered by time\n",
        "# attendance_system.reset_attendance(dummy_student_list)\n",
        "# print(\"\\nAttendance Records After Reset:\")\n",
        "# display(pd.DataFrame.from_dict(attendance_system.get_attendance_records(), orient='index'))"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Simulating face recognition and marking attendance:\n",
            "Attendance marked for student student_001 as Present.\n",
            "Attendance marked for student student_003 as Present.\n",
            "Student student_001 already marked as Present today.\n",
            "Student student_005 not found in the registered list.\n",
            "\n",
            "Attendance Records After Recognition:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "              status                  timestamp  emotion\n",
              "student_000   Absent                        NaT     None\n",
              "student_001  Present 2025-09-25 15:47:00.852214    Happy\n",
              "student_002   Absent                        NaT     None\n",
              "student_003  Present 2025-09-25 15:47:00.852244  Neutral\n",
              "student_004   Absent                        NaT     None"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-c96360aa-e45c-413b-9dcf-1cdd3ab61805\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>status</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>student_000</th>\n",
              "      <td>Absent</td>\n",
              "      <td>NaT</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_001</th>\n",
              "      <td>Present</td>\n",
              "      <td>2025-09-25 15:47:00.852214</td>\n",
              "      <td>Happy</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_002</th>\n",
              "      <td>Absent</td>\n",
              "      <td>NaT</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_003</th>\n",
              "      <td>Present</td>\n",
              "      <td>2025-09-25 15:47:00.852244</td>\n",
              "      <td>Neutral</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_004</th>\n",
              "      <td>Absent</td>\n",
              "      <td>NaT</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c96360aa-e45c-413b-9dcf-1cdd3ab61805')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-c96360aa-e45c-413b-9dcf-1cdd3ab61805 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-c96360aa-e45c-413b-9dcf-1cdd3ab61805');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-e033747b-9d13-44b5-9961-4bb38863a03d\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-e033747b-9d13-44b5-9961-4bb38863a03d')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-e033747b-9d13-44b5-9961-4bb38863a03d button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"# display(pd\",\n  \"rows\": 5,\n  \"fields\": [\n    {\n      \"column\": \"status\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Present\",\n          \"Absent\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"timestamp\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2025-09-25 15:47:00.852214\",\n        \"max\": \"2025-09-25 15:47:00.852244\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"2025-09-25 15:47:00.852244\",\n          \"2025-09-25 15:47:00.852214\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"emotion\",\n      \"properties\": {\n        \"dtype\": \"category\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          \"Neutral\",\n          \"Happy\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8cf99ce2"
      },
      "source": [
        "## Time-based activation\n",
        "\n",
        "### Subtask:\n",
        "Implement a mechanism to activate the system only during the specified time window (9:30 AM to 10:00 AM).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e8271504"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a Python function that checks if the current time is within the specified attendance window (9:30 AM to 10:00 AM) using the `datetime` and `time` modules.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "93cda303"
      },
      "source": [
        "from datetime import datetime, time\n",
        "\n",
        "def is_within_attendance_window():\n",
        "    \"\"\"\n",
        "    Checks if the current time falls within the attendance window (9:30 AM to 10:00 AM).\n",
        "\n",
        "    Returns:\n",
        "        bool: True if the current time is within the window, False otherwise.\n",
        "    \"\"\"\n",
        "    current_time = datetime.now().time()\n",
        "    start_time = time(9, 30)\n",
        "    end_time = time(10, 0)\n",
        "\n",
        "    return start_time <= current_time <= end_time\n",
        "\n",
        "# Example usage (for testing purposes)\n",
        "# print(is_within_attendance_window())"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "138221a8"
      },
      "source": [
        "## Data storage\n",
        "\n",
        "### Subtask:\n",
        "Design a way to store the attendance and emotion data in a structured format like an Excel or CSV file, including the time of identification.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3d8ba5e5"
      },
      "source": [
        "**Reasoning**:\n",
        "Create a function to save attendance records to a CSV file using pandas, including error handling and a confirmation message.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e0ea4ff",
        "outputId": "1d5d1ee5-3254-47d6-cfdb-4715273f9876"
      },
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "def save_attendance_records(attendance_records, filename='attendance_records.csv'):\n",
        "    \"\"\"\n",
        "    Saves attendance records to a CSV file.\n",
        "\n",
        "    Args:\n",
        "        attendance_records (dict): A dictionary containing attendance data.\n",
        "        filename (str): The name of the file to save the data to. Defaults to 'attendance_records.csv'.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Convert the dictionary to a pandas DataFrame\n",
        "        df = pd.DataFrame.from_dict(attendance_records, orient='index')\n",
        "\n",
        "        # Save the DataFrame to a CSV file without the index\n",
        "        df.to_csv(filename, index=False)\n",
        "\n",
        "        print(f\"Attendance records successfully saved to {filename}\")\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"Error saving attendance records to {filename}: {e}\")\n",
        "\n",
        "# Example usage with the dummy data from the previous subtask\n",
        "# Ensure attendance_system and its records exist from the previous step\n",
        "if 'attendance_system' in locals():\n",
        "    current_records = attendance_system.get_attendance_records()\n",
        "    save_attendance_records(current_records)\n",
        "else:\n",
        "    print(\"Attendance system not initialized. Cannot save records.\")\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attendance records successfully saved to attendance_records.csv\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4a93d919"
      },
      "source": [
        "## System integration\n",
        "\n",
        "### Subtask:\n",
        "Combine all the trained models and logic into a single system.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4cfc8fd1"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the necessary libraries and the trained KNN classifier. Define a dummy FaceNet model as the actual one is not available and a dummy emotion detection function. Define the `process_frame` function to integrate face detection, recognition, and emotion detection, and update attendance records if within the attendance window.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9d12a4fa",
        "outputId": "07c094a2-8ee0-4850-cd3f-e7b0d761f469"
      },
      "source": [
        "import joblib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, time\n",
        "import time as time_module # Use a different name to avoid conflict with datetime.time\n",
        "\n",
        "# Load the trained KNN classifier\n",
        "try:\n",
        "    knn_classifier = joblib.load('knn_classifier.pkl')\n",
        "    print(\"KNN classifier loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: knn_classifier.pkl not found. Cannot proceed.\")\n",
        "    knn_classifier = None\n",
        "\n",
        "# Load the FaceNet model (conceptual, as we used a dummy in training)\n",
        "# In a real scenario, you would load your trained or pre-trained FaceNet model here.\n",
        "# For demonstration, we'll create a dummy model that returns random embeddings.\n",
        "def create_dummy_facenet_model():\n",
        "    # This dummy model returns random vectors of size 128\n",
        "    # In a real scenario, this would be your loaded FaceNet model\n",
        "    def dummy_predict(image):\n",
        "        # Simulate batch processing if needed, but here we expect a single image [1, H, W, C]\n",
        "        if image.shape[0] == 1:\n",
        "             return np.random.rand(1, 128)\n",
        "        else:\n",
        "            return np.random.rand(image.shape[0], 128)\n",
        "\n",
        "    class DummyModel:\n",
        "        def predict(self, image):\n",
        "            return dummy_predict(image)\n",
        "    return DummyModel()\n",
        "\n",
        "# Use the create_facenet_model from the previous cell if it exists and is a tf.keras.Model\n",
        "if 'facenet_model' in globals() and isinstance(facenet_model, tf.keras.models.Model):\n",
        "     print(\"Using previously defined FaceNet model.\")\n",
        "     # Ensure the preprocess function is also available\n",
        "     def preprocess(x):\n",
        "        return tf.image.resize(x, (160, 160))\n",
        "else:\n",
        "    print(\"Using dummy FaceNet model.\")\n",
        "    facenet_model = create_dummy_facenet_model()\n",
        "    # Define a dummy preprocess function if the actual one isn't available\n",
        "    def preprocess(x):\n",
        "        return x # No preprocessing for the dummy model\n",
        "\n",
        "# Initialize the face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# Dummy emotion detection function (conceptual)\n",
        "# In a real scenario, you would load and use your trained emotion model here.\n",
        "def detect_emotion(face_image):\n",
        "    \"\"\"\n",
        "    Dummy function to simulate emotion detection.\n",
        "    Returns a random emotion from a predefined list.\n",
        "    \"\"\"\n",
        "    emotions = ['Happy', 'Neutral', 'Sad', 'Angry', 'Surprised']\n",
        "    return np.random.choice(emotions)\n",
        "\n",
        "def process_frame(frame, attendance_manager, is_within_window_func, knn_classifier, facenet_model, detector):\n",
        "    \"\"\"\n",
        "    Processes a single video frame to detect faces, recognize students, and detect emotions.\n",
        "    Updates attendance records if within the attendance window.\n",
        "\n",
        "    Args:\n",
        "        frame (np.array): The video frame as a NumPy array.\n",
        "        attendance_manager (AttendanceManager): The AttendanceManager instance.\n",
        "        is_within_window_func (function): Function to check if within the attendance window.\n",
        "        knn_classifier: The trained KNN classifier model.\n",
        "        facenet_model: The loaded FaceNet model.\n",
        "        detector: The MTCNN face detector.\n",
        "\n",
        "    Returns:\n",
        "        np.array: The frame with bounding boxes, student IDs, and emotions drawn on it.\n",
        "    \"\"\"\n",
        "    if knn_classifier is None:\n",
        "        print(\"KNN classifier not loaded. Skipping frame processing.\")\n",
        "        return frame\n",
        "\n",
        "    # Convert frame to RGB (MTCNN requires RGB)\n",
        "    frame_rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Detect faces in the frame\n",
        "    results = detector.detect_faces(frame_rgb)\n",
        "\n",
        "    # Draw results on the frame and process detected faces\n",
        "    for result in results:\n",
        "        x, y, width, height = result['box']\n",
        "        confidence = result['confidence']\n",
        "\n",
        "        # Draw bounding box\n",
        "        cv2.rectangle(frame, (x, y), (x+width, y+height), (255, 0, 0), 2)\n",
        "\n",
        "        # Extract face region\n",
        "        face_region = frame_rgb[y:y+height, x:x+width]\n",
        "\n",
        "        student_id = \"Unknown\"\n",
        "        emotion = \"Detecting...\"\n",
        "\n",
        "        if face_region.size > 0:\n",
        "            try:\n",
        "                # Preprocess face for FaceNet\n",
        "                face_processed = cv2.resize(face_region, (160, 160))\n",
        "                face_processed = np.expand_dims(face_processed, axis=0) # Add batch dimension\n",
        "\n",
        "                # Get face embedding\n",
        "                embedding = facenet_model.predict(face_processed)\n",
        "\n",
        "                # Predict student ID using the KNN classifier\n",
        "                # Ensure embedding has the correct shape for prediction\n",
        "                if embedding.shape[1] == knn_classifier.n_features_in_:\n",
        "                    predicted_id = knn_classifier.predict(embedding)[0]\n",
        "                    student_id = f'student_{predicted_id:03d}' # Format ID\n",
        "\n",
        "                    # Detect emotion (using the dummy function)\n",
        "                    emotion = detect_emotion(face_region)\n",
        "\n",
        "                    # Mark attendance if within the attendance window\n",
        "                    if is_within_window_func():\n",
        "                        attendance_manager.mark_present(student_id, emotion)\n",
        "                else:\n",
        "                     print(f\"Embedding shape mismatch: Expected {knn_classifier.n_features_in_}, got {embedding.shape[1]}. Skipping recognition.\")\n",
        "\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error during face recognition or emotion detection: {e}\")\n",
        "                student_id = \"Error\"\n",
        "                emotion = \"Error\"\n",
        "\n",
        "        # Display student ID and emotion\n",
        "        label = f\"{student_id} ({emotion})\"\n",
        "        cv2.putText(frame, label, (x, y - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 0, 0), 2)\n",
        "\n",
        "    return frame\n",
        "\n",
        "print(\"process_frame function defined and models/detector initialized.\")"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: knn_classifier.pkl not found. Cannot proceed.\n",
            "Using previously defined FaceNet model.\n",
            "process_frame function defined and models/detector initialized.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3701fa63"
      },
      "source": [
        "**Reasoning**:\n",
        "Since the `knn_classifier.pkl` was not found, the system cannot proceed with the recognition part. The main loop needs to handle this case gracefully. Implement the main loop to simulate capturing frames, process them using `process_frame` (which will skip recognition if the classifier is missing), and include a mechanism to check the time window and save records at the end.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6f15461f",
        "outputId": "20610119-48e4-48db-db9b-016b65ed83d8"
      },
      "source": [
        "import time as time_module # Already imported in the previous block\n",
        "import sys\n",
        "\n",
        "# Assume dummy_student_list and AttendanceManager class are available from previous steps\n",
        "# If not, redefine them for demonstration purposes\n",
        "if 'AttendanceManager' not in globals() or 'dummy_student_list' not in globals():\n",
        "    print(\"AttendanceManager or dummy_student_list not found, creating dummies.\")\n",
        "    class AttendanceManager:\n",
        "        def __init__(self, student_list):\n",
        "            self.attendance_records = {\n",
        "                student_id: {'status': 'Absent', 'timestamp': None, 'emotion': None}\n",
        "                for student_id in student_list\n",
        "            }\n",
        "            self.present_students_today = set()\n",
        "\n",
        "        def mark_present(self, student_id, emotion):\n",
        "            if student_id in self.attendance_records and student_id not in self.present_students_today:\n",
        "                self.attendance_records[student_id]['status'] = 'Present'\n",
        "                self.attendance_records[student_id]['timestamp'] = datetime.now()\n",
        "                self.attendance_records[student_id]['emotion'] = emotion\n",
        "                self.present_students_today.add(student_id)\n",
        "                print(f\"Attendance marked for student {student_id} as Present.\")\n",
        "            elif student_id in self.present_students_today:\n",
        "                print(f\"Student {student_id} already marked as Present today.\")\n",
        "            else:\n",
        "                print(f\"Student {student_id} not found in the registered list.\")\n",
        "\n",
        "        def get_attendance_records(self):\n",
        "            return self.attendance_records\n",
        "\n",
        "    dummy_student_list = [f'student_{i:03d}' for i in range(5)] # Using 5 dummy students\n",
        "\n",
        "# Initialize the AttendanceManager\n",
        "attendance_system = AttendanceManager(dummy_student_list)\n",
        "\n",
        "# Assume is_within_attendance_window and save_attendance_records are available\n",
        "# If not, redefine them\n",
        "if 'is_within_attendance_window' not in globals():\n",
        "     print(\"is_within_attendance_window not found, creating dummy.\")\n",
        "     def is_within_attendance_window():\n",
        "        # Dummy function always returns True for demonstration if the real one is missing\n",
        "        print(\"Using dummy is_within_attendance_window (always True).\")\n",
        "        return True\n",
        "\n",
        "if 'save_attendance_records' not in globals():\n",
        "    print(\"save_attendance_records not found, creating dummy.\")\n",
        "    def save_attendance_records(records, filename='attendance_records.csv'):\n",
        "        print(f\"Dummy: Saving records to {filename}\")\n",
        "        # In a real scenario, this would save to a file\n",
        "        # print(records) # Optionally print dummy save\n",
        "\n",
        "\n",
        "# --- Main loop simulation ---\n",
        "print(\"Starting main loop simulation...\")\n",
        "running = True\n",
        "simulated_frames = 10 # Simulate processing 10 frames\n",
        "\n",
        "# Simulate different times for demonstration\n",
        "# In a real scenario, this would be a video stream\n",
        "simulated_times = [\n",
        "    time(9, 25), # Before window\n",
        "    time(9, 35), # Within window\n",
        "    time(9, 45), # Within window\n",
        "    time(10, 5),  # After window\n",
        "    time(9, 38), # Within window\n",
        "    time(9, 55), # Within window\n",
        "    time(10, 10), # After window\n",
        "    time(9, 40), # Within window\n",
        "    time(9, 32), # Within window\n",
        "    time(10, 1)  # After window\n",
        "]\n",
        "\n",
        "\n",
        "frame_count = 0\n",
        "while running and frame_count < simulated_frames:\n",
        "    print(f\"\\nProcessing simulated frame {frame_count + 1}/{simulated_frames}\")\n",
        "\n",
        "    # Simulate getting a frame (e.g., from a camera)\n",
        "    # Create a dummy frame (black image)\n",
        "    dummy_frame = np.zeros((480, 640, 3), dtype=np.uint8)\n",
        "\n",
        "    # Simulate current time for the check\n",
        "    # In a real loop, you'd just use datetime.now().time()\n",
        "    current_simulated_time = simulated_times[frame_count]\n",
        "    print(f\"Simulated current time: {current_simulated_time.strftime('%H:%M:%S')}\")\n",
        "\n",
        "    # Check if within attendance window\n",
        "    # Temporarily override the real function for simulation if needed\n",
        "    original_is_within_window = is_within_attendance_window\n",
        "    def simulated_is_within_attendance_window():\n",
        "        start_time = time(9, 30)\n",
        "        end_time = time(10, 0)\n",
        "        return start_time <= current_simulated_time <= end_time\n",
        "\n",
        "    is_within_window = simulated_is_within_attendance_window()\n",
        "    print(f\"Is within attendance window? {is_within_window}\")\n",
        "\n",
        "    if is_within_window:\n",
        "        print(\"Within attendance window, processing frame...\")\n",
        "        # In a real system, you would get a frame from the camera here\n",
        "        # processed_frame = process_frame(frame_from_camera, attendance_system, simulated_is_within_attendance_window, knn_classifier, facenet_model, detector)\n",
        "        # For simulation, we'll process the dummy frame, process_frame handles dummy recognition if classifier is missing\n",
        "        processed_frame = process_frame(dummy_frame.copy(), attendance_system, simulated_is_within_attendance_window, knn_classifier, facenet_model, detector)\n",
        "\n",
        "        # Optionally display the processed frame (in a real application)\n",
        "        # cv2.imshow('Attendance System', processed_frame)\n",
        "\n",
        "    else:\n",
        "        print(\"Outside attendance window, skipping frame processing for attendance.\")\n",
        "        # Optionally display the raw frame or a message\n",
        "        # cv2.imshow('Attendance System', dummy_frame)\n",
        "        processed_frame = dummy_frame # No processing for attendance outside window\n",
        "\n",
        "    # Check for key press to exit (e.g., 'q') - conceptual for a real application\n",
        "    # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "    #     running = False\n",
        "\n",
        "    frame_count += 1\n",
        "    time_module.sleep(0.1) # Simulate frame rate\n",
        "\n",
        "# Restore the original function if it was overridden\n",
        "if 'original_is_within_window' in locals():\n",
        "    is_within_attendance_window = original_is_within_window\n",
        "\n",
        "print(\"\\nMain loop simulation finished.\")\n",
        "\n",
        "# Save the final attendance records\n",
        "print(\"Saving final attendance records...\")\n",
        "final_records = attendance_system.get_attendance_records()\n",
        "save_attendance_records(final_records)\n",
        "\n",
        "# Clean up (in a real application)\n",
        "# cv2.destroyAllWindows()\n",
        "\n",
        "print(\"Process completed.\")"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting main loop simulation...\n",
            "\n",
            "Processing simulated frame 1/10\n",
            "Simulated current time: 09:25:00\n",
            "Is within attendance window? False\n",
            "Outside attendance window, skipping frame processing for attendance.\n",
            "\n",
            "Processing simulated frame 2/10\n",
            "Simulated current time: 09:35:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 3/10\n",
            "Simulated current time: 09:45:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 4/10\n",
            "Simulated current time: 10:05:00\n",
            "Is within attendance window? False\n",
            "Outside attendance window, skipping frame processing for attendance.\n",
            "\n",
            "Processing simulated frame 5/10\n",
            "Simulated current time: 09:38:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 6/10\n",
            "Simulated current time: 09:55:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 7/10\n",
            "Simulated current time: 10:10:00\n",
            "Is within attendance window? False\n",
            "Outside attendance window, skipping frame processing for attendance.\n",
            "\n",
            "Processing simulated frame 8/10\n",
            "Simulated current time: 09:40:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 9/10\n",
            "Simulated current time: 09:32:00\n",
            "Is within attendance window? True\n",
            "Within attendance window, processing frame...\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "\n",
            "Processing simulated frame 10/10\n",
            "Simulated current time: 10:01:00\n",
            "Is within attendance window? False\n",
            "Outside attendance window, skipping frame processing for attendance.\n",
            "\n",
            "Main loop simulation finished.\n",
            "Saving final attendance records...\n",
            "Attendance records successfully saved to attendance_records.csv\n",
            "Process completed.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "22e1e797"
      },
      "source": [
        "## Testing and evaluation\n",
        "\n",
        "### Subtask:\n",
        "Thoroughly test the system with new data to ensure accuracy and reliability.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8f102cba"
      },
      "source": [
        "**Reasoning**:\n",
        "Load the necessary models and initialize the AttendanceManager. Then, simulate processing test images and evaluate the system's performance based on expected outcomes.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "b05bbdfe",
        "outputId": "80fc4e21-0eb3-45bd-f5ff-f47aa8a688de"
      },
      "source": [
        "import joblib\n",
        "import cv2\n",
        "import numpy as np\n",
        "from mtcnn.mtcnn import MTCNN\n",
        "import tensorflow as tf\n",
        "from datetime import datetime, time\n",
        "import time as time_module\n",
        "import pandas as pd # Import pandas for displaying results\n",
        "\n",
        "# Ensure the necessary dummy functions and classes are available if not already\n",
        "if 'AttendanceManager' not in globals():\n",
        "    class AttendanceManager:\n",
        "        def __init__(self, student_list):\n",
        "            self.attendance_records = {\n",
        "                student_id: {'status': 'Absent', 'timestamp': None, 'emotion': None}\n",
        "                for student_id in student_list\n",
        "            }\n",
        "            self.present_students_today = set()\n",
        "\n",
        "        def mark_present(self, student_id, emotion):\n",
        "            if student_id in self.attendance_records and student_id not in self.present_students_today:\n",
        "                self.attendance_records[student_id]['status'] = 'Present'\n",
        "                self.attendance_records[student_id]['timestamp'] = datetime.now()\n",
        "                self.attendance_records[student_id]['emotion'] = emotion\n",
        "                self.present_students_today.add(student_id)\n",
        "                # print(f\"Attendance marked for student {student_id} as Present.\")\n",
        "            elif student_id in self.present_students_today:\n",
        "                 pass # print(f\"Student {student_id} already marked as Present today.\")\n",
        "            else:\n",
        "                 pass # print(f\"Student {student_id} not found in the registered list.\")\n",
        "\n",
        "\n",
        "        def get_attendance_records(self):\n",
        "            return self.attendance_records\n",
        "\n",
        "if 'is_within_attendance_window' not in globals():\n",
        "     def is_within_attendance_window():\n",
        "        # Dummy function always returns True for demonstration if the real one is missing\n",
        "        # print(\"Using dummy is_within_attendance_window (always True).\")\n",
        "        return True\n",
        "\n",
        "if 'detect_emotion' not in globals():\n",
        "    def detect_emotion(face_image):\n",
        "        emotions = ['Happy', 'Neutral', 'Sad', 'Angry', 'Surprised']\n",
        "        return np.random.choice(emotions)\n",
        "\n",
        "if 'preprocess' not in globals():\n",
        "     def preprocess(x):\n",
        "        return x # No preprocessing for the dummy model\n",
        "\n",
        "# 1. Load the trained KNN classifier and the FaceNet model. Handle potential FileNotFoundError for the KNN classifier.\n",
        "try:\n",
        "    knn_classifier = joblib.load('knn_classifier.pkl')\n",
        "    print(\"KNN classifier loaded successfully.\")\n",
        "except FileNotFoundError:\n",
        "    print(\"Error: knn_classifier.pkl not found. Cannot perform face recognition testing.\")\n",
        "    knn_classifier = None\n",
        "\n",
        "# Load the FaceNet model (conceptual, as we used a dummy in training)\n",
        "# Use the create_facenet_model from the previous cell if it exists and is a tf.keras.Model\n",
        "if 'facenet_model' in globals() and isinstance(facenet_model, tf.keras.models.Model):\n",
        "     print(\"Using previously defined FaceNet model.\")\n",
        "else:\n",
        "    print(\"Using dummy FaceNet model.\")\n",
        "    def create_dummy_facenet_model():\n",
        "        def dummy_predict(image):\n",
        "            if image.shape[0] == 1:\n",
        "                 return np.random.rand(1, 128)\n",
        "            else:\n",
        "                return np.random.rand(image.shape[0], 128)\n",
        "\n",
        "        class DummyModel:\n",
        "            def predict(self, image):\n",
        "                return dummy_predict(image)\n",
        "        return DummyModel()\n",
        "    facenet_model = create_dummy_facenet_model()\n",
        "\n",
        "\n",
        "# Initialize the face detector\n",
        "detector = MTCNN()\n",
        "\n",
        "# 2. Initialize the AttendanceManager with a list of known student IDs.\n",
        "known_student_ids = [f'student_{i:03d}' for i in range(5)] # Assuming 5 known students\n",
        "attendance_manager = AttendanceManager(known_student_ids)\n",
        "print(f\"AttendanceManager initialized with {len(known_student_ids)} known students.\")\n",
        "\n",
        "# 3. Prepare a set of test images\n",
        "# Create dummy test images and their expected outcomes\n",
        "test_data = [\n",
        "    {'image': np.zeros((480, 640, 3), dtype=np.uint8), 'time': time(9, 35), 'expected_detection': False, 'expected_recognition': None, 'expected_emotion': None, 'expected_attendance': 'Absent'}, # No face, within window\n",
        "    {'image': cv2.circle(np.zeros((480, 640, 3), dtype=np.uint8), (320, 240), 100, (255, 255, 255), -1), 'time': time(9, 40), 'expected_detection': True, 'expected_recognition': 'student_001', 'expected_emotion': 'Happy', 'expected_attendance': 'Present'}, # Dummy face, within window, known student 1\n",
        "    {'image': cv2.circle(np.zeros((480, 640, 3), dtype=np.uint8), (320, 240), 100, (255, 255, 255), -1), 'time': time(10, 5), 'expected_detection': True, 'expected_recognition': 'student_002', 'expected_emotion': 'Neutral', 'expected_attendance': 'Absent'}, # Dummy face, outside window, known student 2\n",
        "    {'image': cv2.circle(np.zeros((480, 640, 3), dtype=np.uint8), (320, 240), 100, (255, 255, 255), -1), 'time': time(9, 45), 'expected_detection': True, 'expected_recognition': 'Unknown', 'expected_emotion': 'Sad', 'expected_attendance': 'Absent'}, # Dummy face, within window, unknown student\n",
        "    {'image': cv2.circle(np.zeros((480, 640, 3), dtype=np.uint8), (150, 200), 80, (255, 255, 255), -1), 'time': time(9, 50), 'expected_detection': True, 'expected_recognition': 'student_003', 'expected_emotion': 'Surprised', 'expected_attendance': 'Present'}, # Dummy face, within window, known student 3, different position\n",
        "]\n",
        "\n",
        "# Add more complex dummy faces for better simulation if possible with simple shapes\n",
        "def create_more_complex_dummy_face(img_height, img_width, center_x, center_y):\n",
        "    img = np.zeros((img_height, img_width, 3), dtype=np.uint8)\n",
        "    # Draw a circle for the head\n",
        "    cv2.circle(img, (center_x, center_y), min(img_height, img_width) // 6, (255, 255, 255), -1)\n",
        "    # Draw eyes\n",
        "    eye_radius = max(1, min(img_height, img_width) // 30)\n",
        "    eye_offset_x = min(img_width // 12, center_x - eye_radius - 1)\n",
        "    eye_offset_y = min(img_height // 12, center_y - eye_radius - 1)\n",
        "    cv2.circle(img, (center_x - eye_offset_x, center_y - eye_offset_y), eye_radius, (0, 0, 0), -1)\n",
        "    cv2.circle(img, (center_x + eye_offset_x, center_y - eye_offset_y), eye_radius, (0, 0, 0), -1)\n",
        "    # Draw mouth (simple line)\n",
        "    mouth_width = min(img_width // 8, center_x - 10)\n",
        "    mouth_height_offset = min(img_height // 16, center_y - 10)\n",
        "    cv2.line(img, (center_x - mouth_width // 2, center_y + mouth_height_offset),\n",
        "             (center_x + mouth_width // 2, center_y + mouth_height_offset), (0, 0, 0), max(1, min(img_height, img_width) // 60))\n",
        "    return img\n",
        "\n",
        "test_data.append({'image': create_more_complex_dummy_face(480, 640, 450, 300), 'time': time(9, 58), 'expected_detection': True, 'expected_recognition': 'student_004', 'expected_emotion': 'Angry', 'expected_attendance': 'Present'}) # More complex dummy face, within window, known student 4\n",
        "test_data.append({'image': create_more_complex_dummy_face(480, 640, 200, 400), 'time': time(10, 15), 'expected_detection': True, 'expected_recognition': 'student_000', 'expected_emotion': 'Happy', 'expected_attendance': 'Absent'}) # More complex dummy face, outside window, known student 0\n",
        "\n",
        "\n",
        "# Counters for evaluation\n",
        "detection_results = {'detected': 0, 'not_detected': 0, 'correct_detection': 0, 'incorrect_detection': 0}\n",
        "recognition_results = {'correct_recognition': 0, 'incorrect_recognition': 0, 'unknown_correctly_flagged': 0, 'unknown_incorrectly_identified': 0}\n",
        "attendance_marking_results = {'correct_attendance': 0, 'incorrect_attendance': 0}\n",
        "emotion_detection_acknowledgement = \"Emotion detection is simulated using a dummy function, so accuracy cannot be truly evaluated.\"\n",
        "\n",
        "# 4. For each test image:\n",
        "print(\"\\nStarting system testing with simulated data...\")\n",
        "for i, test_case in enumerate(test_data):\n",
        "    print(f\"\\nProcessing Test Case {i + 1}\")\n",
        "    frame = test_case['image']\n",
        "    simulated_time = test_case['time']\n",
        "    expected_detection = test_case['expected_detection']\n",
        "    expected_recognition = test_case['expected_recognition']\n",
        "    expected_emotion = test_case['expected_emotion']\n",
        "    expected_attendance = test_case['expected_attendance']\n",
        "\n",
        "    # Simulate current time for the check\n",
        "    original_is_within_window = is_within_attendance_window\n",
        "    def simulated_is_within_attendance_window():\n",
        "        start_time = time(9, 30)\n",
        "        end_time = time(10, 0)\n",
        "        return start_time <= simulated_time <= end_time\n",
        "\n",
        "    is_within_window = simulated_is_within_attendance_window()\n",
        "    print(f\"Simulated time: {simulated_time.strftime('%H:%M:%S')}, Within attendance window? {is_within_window}\")\n",
        "\n",
        "    detected_faces = False\n",
        "    recognized_id = None\n",
        "    detected_emotion = None\n",
        "    attendance_status_before = attendance_manager.get_attendance_records().get(expected_recognition, {}).get('status', 'Absent')\n",
        "\n",
        "    if is_within_window or expected_detection: # Process frame if within window or if we expect a detection to happen\n",
        "         # Process the image using the process_frame function.\n",
        "        processed_frame = process_frame(frame.copy(), attendance_manager, simulated_is_within_attendance_window, knn_classifier, facenet_model, detector)\n",
        "\n",
        "        # Analyze results from process_frame (this is simplified for simulation)\n",
        "        # In a real system, process_frame would return detected faces, IDs, emotions\n",
        "        # For this simulation, we'll re-run face detection and use the internal state of AttendanceManager\n",
        "\n",
        "        results = detector.detect_faces(cv2.cvtColor(frame, cv2.COLOR_BGR2RGB))\n",
        "        if results:\n",
        "            detected_faces = True\n",
        "            detection_results['detected'] += 1\n",
        "\n",
        "            # Assuming the first detected face is the one we evaluate for recognition and emotion\n",
        "            x, y, width, height = results[0]['box']\n",
        "            face_region = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)[y:y+height, x:x+width]\n",
        "\n",
        "            if face_region.size > 0 and knn_classifier is not None:\n",
        "                try:\n",
        "                    face_processed = cv2.resize(face_region, (160, 160))\n",
        "                    face_processed = np.expand_dims(face_processed, axis=0)\n",
        "                    embedding = facenet_model.predict(face_processed)\n",
        "\n",
        "                    if embedding.shape[1] == knn_classifier.n_features_in_:\n",
        "                        predicted_id_idx = knn_classifier.predict(embedding)[0]\n",
        "                        recognized_id = f'student_{predicted_id_idx:03d}'\n",
        "                        detected_emotion = detect_emotion(face_region) # Get a dummy emotion\n",
        "                    else:\n",
        "                         print(f\"Embedding shape mismatch for test case {i+1}. Skipping recognition.\")\n",
        "\n",
        "\n",
        "                except Exception as e:\n",
        "                    print(f\"Error during simulation recognition/emotion for test case {i+1}: {e}\")\n",
        "\n",
        "\n",
        "        else:\n",
        "            detection_results['not_detected'] += 1\n",
        "            print(\"No faces detected in the simulated frame.\")\n",
        "\n",
        "    else:\n",
        "         print(\"Outside attendance window, skipping face processing for this frame.\")\n",
        "\n",
        "\n",
        "    # 5. Keep track of the evaluation results\n",
        "    # Evaluate Face Detection\n",
        "    if detected_faces == expected_detection:\n",
        "        detection_results['correct_detection'] += 1\n",
        "        print(f\"Face Detection: Correct (Detected: {detected_faces}, Expected: {expected_detection})\")\n",
        "    else:\n",
        "        detection_results['incorrect_detection'] += 1\n",
        "        print(f\"Face Detection: Incorrect (Detected: {detected_faces}, Expected: {expected_detection})\")\n",
        "\n",
        "    # Evaluate Face Recognition (only if a face was detected and classifier is available)\n",
        "    if detected_faces and knn_classifier is not None:\n",
        "        if expected_recognition == 'Unknown':\n",
        "            if recognized_id is None or recognized_id not in known_student_ids:\n",
        "                recognition_results['unknown_correctly_flagged'] += 1\n",
        "                print(f\"Face Recognition: Correct (Unknown correctly flagged)\")\n",
        "            else:\n",
        "                recognition_results['unknown_incorrectly_identified'] += 1\n",
        "                print(f\"Face Recognition: Incorrect (Unknown incorrectly identified as {recognized_id})\")\n",
        "        else: # Expected to be a known student\n",
        "            if recognized_id == expected_recognition:\n",
        "                recognition_results['correct_recognition'] += 1\n",
        "                print(f\"Face Recognition: Correct (Identified as {recognized_id}, Expected: {expected_recognition})\")\n",
        "            else:\n",
        "                recognition_results['incorrect_recognition'] += 1\n",
        "                print(f\"Face Recognition: Incorrect (Identified as {recognized_id}, Expected: {expected_recognition})\")\n",
        "    elif detected_faces and knn_classifier is None:\n",
        "         print(\"Face Recognition: Skipped due to missing KNN classifier.\")\n",
        "    elif not detected_faces and expected_recognition is not None:\n",
        "         print(f\"Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: {expected_recognition}).\")\n",
        "\n",
        "\n",
        "    # Evaluate Attendance Marking\n",
        "    attendance_status_after = attendance_manager.get_attendance_records().get(expected_recognition, {}).get('status', 'Absent')\n",
        "\n",
        "    # Determine expected final status based on initial status, window, and expected recognition\n",
        "    if expected_recognition in known_student_ids and is_within_window and attendance_status_before == 'Absent':\n",
        "         expected_final_attendance = 'Present'\n",
        "    else:\n",
        "         expected_final_attendance = attendance_status_before # Should remain the same or Absent if unknown\n",
        "\n",
        "    # Check if the status in the attendance_manager matches the expected final status\n",
        "    # Note: The dummy recognition and emotion might affect this, but we check the overall logic\n",
        "    if attendance_status_after == expected_final_attendance:\n",
        "        attendance_marking_results['correct_attendance'] += 1\n",
        "        print(f\"Attendance Marking: Correct (Status after: {attendance_status_after}, Expected: {expected_final_attendance})\")\n",
        "    else:\n",
        "        attendance_marking_results['incorrect_attendance'] += 1\n",
        "        print(f\"Attendance Marking: Incorrect (Status after: {attendance_status_after}, Expected: {expected_final_attendance})\")\n",
        "\n",
        "\n",
        "# Restore the original function if it was overridden\n",
        "if 'original_is_within_window' in locals():\n",
        "    is_within_attendance_window = original_is_within_window\n",
        "\n",
        "# 6. Summarize the testing and evaluation results\n",
        "print(\"\\n--- Testing Summary ---\")\n",
        "print(\"Face Detection Results:\")\n",
        "display(pd.DataFrame([detection_results]))\n",
        "\n",
        "print(\"\\nFace Recognition Results:\")\n",
        "# Only display if KNN classifier was available\n",
        "if knn_classifier is not None:\n",
        "    display(pd.DataFrame([recognition_results]))\n",
        "else:\n",
        "    print(\"Face recognition testing skipped due to missing KNN classifier.\")\n",
        "\n",
        "print(\"\\nAttendance Marking Results:\")\n",
        "display(pd.DataFrame([attendance_marking_results]))\n",
        "\n",
        "print(\"\\nEmotion Detection:\")\n",
        "print(emotion_detection_acknowledgement)\n",
        "\n",
        "print(\"\\nFinal Attendance Records After Testing:\")\n",
        "display(pd.DataFrame.from_dict(attendance_manager.get_attendance_records(), orient='index'))"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Error: knn_classifier.pkl not found. Cannot perform face recognition testing.\n",
            "Using previously defined FaceNet model.\n",
            "AttendanceManager initialized with 5 known students.\n",
            "\n",
            "Starting system testing with simulated data...\n",
            "\n",
            "Processing Test Case 1\n",
            "Simulated time: 09:35:00, Within attendance window? True\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Correct (Detected: False, Expected: False)\n",
            "Attendance Marking: Correct (Status after: Absent, Expected: Absent)\n",
            "\n",
            "Processing Test Case 2\n",
            "Simulated time: 09:40:00, Within attendance window? True\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: student_001).\n",
            "Attendance Marking: Incorrect (Status after: Absent, Expected: Present)\n",
            "\n",
            "Processing Test Case 3\n",
            "Simulated time: 10:05:00, Within attendance window? False\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: student_002).\n",
            "Attendance Marking: Correct (Status after: Absent, Expected: Absent)\n",
            "\n",
            "Processing Test Case 4\n",
            "Simulated time: 09:45:00, Within attendance window? True\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: Unknown).\n",
            "Attendance Marking: Correct (Status after: Absent, Expected: Absent)\n",
            "\n",
            "Processing Test Case 5\n",
            "Simulated time: 09:50:00, Within attendance window? True\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: student_003).\n",
            "Attendance Marking: Incorrect (Status after: Absent, Expected: Present)\n",
            "\n",
            "Processing Test Case 6\n",
            "Simulated time: 09:58:00, Within attendance window? True\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: student_004).\n",
            "Attendance Marking: Incorrect (Status after: Absent, Expected: Present)\n",
            "\n",
            "Processing Test Case 7\n",
            "Simulated time: 10:15:00, Within attendance window? False\n",
            "KNN classifier not loaded. Skipping frame processing.\n",
            "No faces detected in the simulated frame.\n",
            "Face Detection: Incorrect (Detected: False, Expected: True)\n",
            "Face Recognition: Cannot evaluate recognition as no face was detected (Expected recognition: student_000).\n",
            "Attendance Marking: Correct (Status after: Absent, Expected: Absent)\n",
            "\n",
            "--- Testing Summary ---\n",
            "Face Detection Results:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   detected  not_detected  correct_detection  incorrect_detection\n",
              "0         0             7                  1                    6"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-bc002150-0d08-45ff-866c-8f45c73e7d36\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>detected</th>\n",
              "      <th>not_detected</th>\n",
              "      <th>correct_detection</th>\n",
              "      <th>incorrect_detection</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>7</td>\n",
              "      <td>1</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-bc002150-0d08-45ff-866c-8f45c73e7d36')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-bc002150-0d08-45ff-866c-8f45c73e7d36 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-bc002150-0d08-45ff-866c-8f45c73e7d36');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"detected\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"not_detected\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 7,\n        \"max\": 7,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          7\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"correct_detection\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect_detection\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 6,\n        \"max\": 6,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          6\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Face Recognition Results:\n",
            "Face recognition testing skipped due to missing KNN classifier.\n",
            "\n",
            "Attendance Marking Results:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "   correct_attendance  incorrect_attendance\n",
              "0                   4                     3"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-36c86be3-1051-444d-bfa2-847d551198c1\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>correct_attendance</th>\n",
              "      <th>incorrect_attendance</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>4</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-36c86be3-1051-444d-bfa2-847d551198c1')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-36c86be3-1051-444d-bfa2-847d551198c1 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-36c86be3-1051-444d-bfa2-847d551198c1');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"display(pd\",\n  \"rows\": 1,\n  \"fields\": [\n    {\n      \"column\": \"correct_attendance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 4,\n        \"max\": 4,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"incorrect_attendance\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": null,\n        \"min\": 3,\n        \"max\": 3,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          3\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Emotion Detection:\n",
            "Emotion detection is simulated using a dummy function, so accuracy cannot be truly evaluated.\n",
            "\n",
            "Final Attendance Records After Testing:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "             status timestamp emotion\n",
              "student_000  Absent      None    None\n",
              "student_001  Absent      None    None\n",
              "student_002  Absent      None    None\n",
              "student_003  Absent      None    None\n",
              "student_004  Absent      None    None"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-4bff137f-c271-48b7-a40f-f13ea523f76f\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>status</th>\n",
              "      <th>timestamp</th>\n",
              "      <th>emotion</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>student_000</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_001</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_002</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_003</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>student_004</th>\n",
              "      <td>Absent</td>\n",
              "      <td>None</td>\n",
              "      <td>None</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-4bff137f-c271-48b7-a40f-f13ea523f76f')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-4bff137f-c271-48b7-a40f-f13ea523f76f button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-4bff137f-c271-48b7-a40f-f13ea523f76f');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-43b24c1f-0879-4c41-8474-5c0cb8f00994\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-43b24c1f-0879-4c41-8474-5c0cb8f00994')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-43b24c1f-0879-4c41-8474-5c0cb8f00994 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "repr_error": "Out of range float values are not JSON compliant: nan"
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e3a0b5fc"
      },
      "source": [
        "## Refinement and deployment\n",
        "\n",
        "### Subtask:\n",
        "Based on testing, refine the models and system as needed, and prepare for deployment.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6234832e"
      },
      "source": [
        "**Reasoning**:\n",
        "Acknowledge the limitations found during testing, state the need for a real dataset for refinement, explain why refinement is currently blocked, outline the necessary steps with a real dataset, discuss deployment considerations, and conclude that further refinement is contingent on real data.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9a473d1e",
        "outputId": "3cbabf9d-a219-4052-dc57-4e42784c2eeb"
      },
      "source": [
        "# 1. Acknowledge limitations from testing.\n",
        "print(\"--- Post-Testing Refinement ---\")\n",
        "print(\"Based on the testing phase, significant limitations were identified.\")\n",
        "print(\"Specifically, the face detection model (MTCNN) failed to detect faces in the dummy images used for testing.\")\n",
        "print(\"Additionally, the KNN classifier file ('knn_classifier.pkl') was not found, which prevented meaningful evaluation of face recognition and subsequent attendance marking logic.\")\n",
        "print(\"\\nThese issues indicate that the current models are not functional with the provided test data and setup.\")\n",
        "\n",
        "# 2. State the crucial need for a real dataset.\n",
        "print(\"\\nA crucial refinement step requires a real dataset of student images to properly train and test the face detection, recognition, and emotion models.\")\n",
        "\n",
        "# 3. Explain why refinement is blocked without real data.\n",
        "print(\"Without successful face detection and recognition on real data, the attendance marking based on these outputs, and thus the overall system's performance, cannot be accurately evaluated or refined.\")\n",
        "print(\"The dummy data and missing model prevented any real assessment of the system's core functions.\")\n",
        "\n",
        "# 4. Outline necessary steps once a real dataset is available.\n",
        "print(\"\\nOnce a real dataset is available, the following steps are necessary for refinement:\")\n",
        "print(\" - Retrain the face detection model (or fine-tune a pre-trained one) using the real student dataset.\")\n",
        "print(\" - Retrain the face recognition model (FaceNet + classifier) using the embeddings extracted from the real student dataset.\")\n",
        "print(\" - Train or fine-tune the emotion detection model using the emotion labels in the real student dataset.\")\n",
        "print(\" - Re-run the system integration and testing phases with the newly trained models and real test data.\")\n",
        "print(\" - Based on the results of testing with real data, identify areas for further refinement (e.g., improving model accuracy, handling edge cases, optimizing performance).\")\n",
        "\n",
        "# 5. Discuss potential considerations for deployment.\n",
        "print(\"\\nPotential considerations for deployment, once the models are sufficiently accurate and reliable, include:\")\n",
        "print(\" - Target environment (e.g., cloud server for centralized processing, edge device like a Raspberry Pi for local processing).\")\n",
        "print(\" - Required hardware (e.g., suitable camera with sufficient resolution, processing unit capable of running the models in real-time).\")\n",
        "print(\" - Necessary software dependencies (e.g., specific versions of TensorFlow, OpenCV, other libraries, operating system compatibility).\")\n",
        "print(\" - System latency and throughput requirements for real-time performance.\")\n",
        "print(\" - Data privacy and security considerations for handling student images and attendance records.\")\n",
        "\n",
        "# 6. Conclude on the current readiness for deployment.\n",
        "print(\"\\nIn conclusion, significant refinement is needed before the system can be considered ready for deployment.\")\n",
        "print(\"This refinement is entirely contingent upon obtaining and utilizing a real dataset for comprehensive model training, testing, and evaluation.\")"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--- Post-Testing Refinement ---\n",
            "Based on the testing phase, significant limitations were identified.\n",
            "Specifically, the face detection model (MTCNN) failed to detect faces in the dummy images used for testing.\n",
            "Additionally, the KNN classifier file ('knn_classifier.pkl') was not found, which prevented meaningful evaluation of face recognition and subsequent attendance marking logic.\n",
            "\n",
            "These issues indicate that the current models are not functional with the provided test data and setup.\n",
            "\n",
            "A crucial refinement step requires a real dataset of student images to properly train and test the face detection, recognition, and emotion models.\n",
            "Without successful face detection and recognition on real data, the attendance marking based on these outputs, and thus the overall system's performance, cannot be accurately evaluated or refined.\n",
            "The dummy data and missing model prevented any real assessment of the system's core functions.\n",
            "\n",
            "Once a real dataset is available, the following steps are necessary for refinement:\n",
            " - Retrain the face detection model (or fine-tune a pre-trained one) using the real student dataset.\n",
            " - Retrain the face recognition model (FaceNet + classifier) using the embeddings extracted from the real student dataset.\n",
            " - Train or fine-tune the emotion detection model using the emotion labels in the real student dataset.\n",
            " - Re-run the system integration and testing phases with the newly trained models and real test data.\n",
            " - Based on the results of testing with real data, identify areas for further refinement (e.g., improving model accuracy, handling edge cases, optimizing performance).\n",
            "\n",
            "Potential considerations for deployment, once the models are sufficiently accurate and reliable, include:\n",
            " - Target environment (e.g., cloud server for centralized processing, edge device like a Raspberry Pi for local processing).\n",
            " - Required hardware (e.g., suitable camera with sufficient resolution, processing unit capable of running the models in real-time).\n",
            " - Necessary software dependencies (e.g., specific versions of TensorFlow, OpenCV, other libraries, operating system compatibility).\n",
            " - System latency and throughput requirements for real-time performance.\n",
            " - Data privacy and security considerations for handling student images and attendance records.\n",
            "\n",
            "In conclusion, significant refinement is needed before the system can be considered ready for deployment.\n",
            "This refinement is entirely contingent upon obtaining and utilizing a real dataset for comprehensive model training, testing, and evaluation.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "05941589"
      },
      "source": [
        "## Summary:\n",
        "\n",
        "### Data Analysis Key Findings\n",
        "\n",
        "*   **Dataset:** A conceptual plan for collecting, preparing, and organizing a custom dataset of student images with identity and emotion labels was detailed. The need for 50-100 images per student covering variations in lighting, pose, and expression was highlighted.\n",
        "*   **Face Detection & Recognition:** An outline for training face detection (using MTCNN) and recognition (using FaceNet embeddings and a KNN classifier) models was created. However, attempts to train the KNN classifier using dummy data failed because the MTCNN detector could not find faces in the artificial images.\n",
        "*   **Emotion Detection:** A conceptual outline for training an emotion detection model using CNNs and pre-trained weights was successfully created, detailing steps for data preparation, training (including fine-tuning), handling class imbalance, and evaluation.\n",
        "*   **Attendance Logic:** A Python class `AttendanceManager` was developed to manage student attendance records, including marking students as present (only once per session), storing timestamps and emotions, and handling unrecognized students. This logic was demonstrated through a simulation.\n",
        "*   **Time-Based Activation:** A function `is_within_attendance_window` was implemented to check if the current time falls within the specified 9:30 AM to 10:00 AM window.\n",
        "*   **Data Storage:** A function `save_attendance_records` was created using pandas to save the attendance and emotion data to a CSV file.\n",
        "*   **System Integration:** A simulation of the integrated system was created, combining face detection (MTCNN), conceptual face recognition (dummy FaceNet + KNN, though the KNN model file was missing), dummy emotion detection, time-based activation, and attendance management. The simulation showed the system checking the time window and attempting to process frames.\n",
        "*   **Testing & Evaluation:** A testing phase was conducted using simulated data. However, the tests revealed critical issues: the MTCNN detector failed to detect faces in the dummy test images, and the KNN classifier file was not found, preventing meaningful face recognition and attendance marking evaluation. The testing process confirmed the time window check logic worked in the simulation.\n",
        "*   **Refinement & Deployment:** The refinement process is contingent on obtaining a real student dataset to properly train and test the models. Without successful face detection and recognition on real data, further refinement and accurate evaluation are blocked. Deployment considerations were outlined conceptually, including environment, hardware, software, performance, privacy, and security.\n",
        "\n",
        "### Insights or Next Steps\n",
        "\n",
        "*   The primary blocker for building a functional system is the lack of a real, diverse dataset of student images. Acquiring this dataset is the immediate and most crucial next step.\n",
        "*   Once a real dataset is available, the face detection, recognition, and emotion detection models must be trained or fine-tuned using this data. Subsequently, the integrated system should be thoroughly tested with real images and in realistic conditions to evaluate accuracy and reliability before further refinement and deployment.\n"
      ]
    }
  ]
}